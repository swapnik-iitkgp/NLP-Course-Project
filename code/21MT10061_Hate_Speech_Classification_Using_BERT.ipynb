{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob\n",
    "# !pip install gensim\n",
    "# nltk.download('stopwords')\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import Word\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the training data\n",
    "\n",
    "data_folder_path = \"C:\\\\Users\\\\swapn\\\\OneDrive\\\\Desktop\\\\NLP\\\\Assignment2\\\\NLP_AUTUMN_ASSIGNMENT_DATA\\\\\"\n",
    "\n",
    "train_file_path = data_folder_path + \"NLP_ass_train.tsv\"\n",
    "val_file_path = data_folder_path + \"NLP_ass_valid.tsv\"\n",
    "test_file_path = data_folder_path + \"NLP_ass_test.tsv\"\n",
    "\n",
    "train_df = pd.read_csv(train_file_path, delimiter='\\t')\n",
    "val_df = pd.read_csv(val_file_path, delimiter='\\t')\n",
    "test_df = pd.read_csv(test_file_path, delimiter='\\t')\n",
    "\n",
    "\n",
    "# Renaming the columns\n",
    "df1 = pd.DataFrame({train_df.columns[0]: [train_df.columns[0]], train_df.columns[1]: [train_df.columns[1]]})\n",
    "train_df = pd.concat([df1, train_df])\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df.columns = [\"Text\", \"Label\"]\n",
    "\n",
    "df2 = pd.DataFrame({val_df.columns[0]: [val_df.columns[0]], val_df.columns[1]: [val_df.columns[1]]})\n",
    "val_df = pd.concat([df2, val_df])\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "val_df.columns = [\"Text\", \"Label\"]\n",
    "\n",
    "df3 = pd.DataFrame({test_df.columns[0]: [test_df.columns[0]], test_df.columns[1]: [test_df.columns[1]]})\n",
    "test_df = pd.concat([df3, test_df])\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "test_df.columns = [\"Text\", \"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and symbols\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Handling Negations\n",
    "    # Detect and handle negations by appending \"not_\" to the words that follow negation terms.\n",
    "    words = text.split()\n",
    "    for i in range(len(words)):\n",
    "        if words[i] == 'not' and i < len(words) - 1:\n",
    "            words[i + 1] = 'not_' + words[i + 1]\n",
    "    \n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    # Tokenize the text (split it into words)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    # Token Normalization\n",
    "    # Normalize words by reducing them to their base forms.\n",
    "    words = text.split()\n",
    "    normalized_words = [Word(word).lemmatize() for word in words]\n",
    "    \n",
    "    # Join the words back into a cleaned text\n",
    "    cleaned_text = ' '.join(normalized_words)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "train_df['Text'] = train_df['Text'].apply(preprocess_text)\n",
    "val_df['Text'] = val_df['Text'].apply(preprocess_text)\n",
    "test_df['Text'] = test_df['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train = train_df[train_df.columns[0]].tolist()\n",
    "X_val = val_df[val_df.columns[0]].tolist()\n",
    "X_test = test_df[test_df.columns[0]].tolist()\n",
    "y_train = train_df[train_df.columns[1]].tolist()\n",
    "y_val = val_df[val_df.columns[1]].tolist()\n",
    "y_test = test_df[test_df.columns[1]].tolist()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Suppose 'train_labels' is a list of string labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val = label_encoder.fit_transform(y_val)\n",
    "y_test = label_encoder.fit_transform(y_test)\n",
    "\n",
    "\n",
    "# Encode the labels using LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_train = label_encoder.fit_transform(y_train)\n",
    "# y_val = label_encoder.fit_transform(y_val)\n",
    "# y_test = label_encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 105\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentences and find their lengths\n",
    "train_tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in X_train]\n",
    "lengths = [len(tokens) for tokens in train_tokenized_sentences]\n",
    "\n",
    "# Find the maximum length\n",
    "max_length = max(lengths)\n",
    "\n",
    "print(\"Maximum length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 81\n"
     ]
    }
   ],
   "source": [
    "val_tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in X_val]\n",
    "lengths = [len(tokens) for tokens in val_tokenized_sentences]\n",
    "\n",
    "# Find the maximum length\n",
    "val_max_length = max(lengths)\n",
    "\n",
    "print(\"Maximum length:\", val_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 69\n"
     ]
    }
   ],
   "source": [
    "test_tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in X_test]\n",
    "lengths = [len(tokens) for tokens in test_tokenized_sentences]\n",
    "\n",
    "# Find the maximum length\n",
    "test_max_length = max(lengths)\n",
    "\n",
    "print(\"Maximum length:\", test_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Suppose 'train_labels' is a list of string labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val = label_encoder.fit_transform(y_val)\n",
    "y_test = label_encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in training set: {0, 1, 2}\n",
      "Unique labels in validation set: {0, 1, 2}\n",
      "Unique labels in test set: {0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique labels in training set:\", set(y_train))\n",
    "print(\"Unique labels in validation set:\", set(y_val))\n",
    "print(\"Unique labels in test set:\", set(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train, tokenizer, max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(X_val, y_val, tokenizer, val_max_length)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(X_test, y_test, tokenizer, test_max_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import Word\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "\n",
    "# Model configuration and training loop\n",
    "class CustomBERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, dropout_prob=0.1):\n",
    "        super(CustomBERTClassifier, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomBERTClassifier(\n",
       "  (bert): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model, optimizer, scheduler, and criterion\n",
    "model = CustomBERTClassifier(num_labels=num_classes)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "num_train_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=1500, num_training_steps=num_train_steps)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–Ž         | 18/481 [18:02<7:36:35, 59.17s/it] "
     ]
    }
   ],
   "source": [
    "best_accuracy = 0.0\n",
    "n_no_improve = 0\n",
    "patience = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_accuracy = evaluate(model, val_loader)  # Implement your validation evaluation function\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Accuracy: {validation_accuracy}\")\n",
    "\n",
    "    if validation_accuracy > best_accuracy:\n",
    "        best_accuracy = validation_accuracy\n",
    "        n_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"BERT_Model.pth\")\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    if n_no_improve >= patience:\n",
    "        break\n",
    "\n",
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load(\"BERT_Model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = evaluate(model, test_loader)  # Implement your test evaluation function\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "# Calculate the macro F1 score\n",
    "macro_f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "print(\"Macro F1 Score:\", macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_intersection(set1, set2):\n",
    "    return len(set(set1) & set(set2))\n",
    "\n",
    "# Example train, validation, and test sets (replace with your actual data)\n",
    "train_set = set(train_df[\"Text\"])\n",
    "validation_set = set(val_df[\"Text\"])\n",
    "test_set = set(test_df[\"Text\"])\n",
    "\n",
    "# Calculate and print the intersections\n",
    "intersection_train_validation = calculate_intersection(train_set, validation_set)\n",
    "intersection_train_test = calculate_intersection(train_set, test_set)\n",
    "intersection_validation_test = calculate_intersection(validation_set, test_set)\n",
    "\n",
    "print(\"Intersection between train and validation sets:\", intersection_train_validation)\n",
    "print(\"Intersection between train and test sets:\", intersection_train_test)\n",
    "print(\"Intersection between validation and test sets:\", intersection_validation_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
